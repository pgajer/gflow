## ============================================================================
## Bayesian Mixture Model for Local Slope Association Testing
## ============================================================================
##
## This module implements an empirical Bayes approach to testing local
## associations between a smoothed response y.hat and smoothed features z.hat
## using the lslope() measure. The framework:
##
## 1. Generates a null distribution via permutation of the ORIGINAL y
##    (not the smoothed y.hat), then smoothing the permuted response
## 2. Fits a two-component mixture model (null + alternative)
## 3. Computes vertex-wise posterior probabilities P(null | observed lslope)
##
## The approach follows Efron's empirical Bayes framework for large-scale
## inference, adapted to the geometric data analysis setting where associations
## may vary spatially across the graph.
##
## The null distribution is generated by permuting y before smoothing. This
## ensures that the null properly captures what lslope values would look like
## when there's no true signal but the geometric smoothing is still applied.
##
## @references
## Efron, B. (2008). Microarrays, empirical Bayes and the two-groups model.
## Statistical Science, 23(1), 1-22.
## ============================================================================


## ============================================================================
## STEP 1: GENERATE NULL DISTRIBUTION VIA PERMUTATION
## ============================================================================

#' Generate Null Distribution for lslope via Permutation
#'
#' Characterizes the null distribution of lslope() values when there is no
#' true association between y and Z. This is done by:
#' 1. Permuting the ORIGINAL response y (breaking true associations)
#' 2. Smoothing the permuted response to get y.hat.perm
#' 3. Computing lslope(y.hat.perm, z.hat) for each feature
#'
#' This approach ensures that the null distribution properly accounts for
#' the geometric smoothing process, capturing any spurious structure that
#' arises from smoothing alone.
#'
#' @param fitted.model Fitted model object from \code{fit.rdgraph.regression()}.
#'   Must contain the spectral decomposition for efficient re-smoothing.
#' @param y Original (unsmoothed) response vector (length n). This is what
#'   gets permuted to generate the null distribution.
#' @param Z.hat Matrix of smoothed feature values (n x p). These remain fixed
#'   across permutations since we're testing association with y.
#' @param n.perm Number of permutations (default 200)
#' @param lslope.type Type of lslope measure: "slope", "normalized", or "sign"
#' @param ascending Logical. Use ascending gradient direction (default TRUE)
#' @param pool.features Logical. Pool null values across features (default TRUE)
#' @param per.column.gcv Logical. If TRUE, select optimal eta for each
#'   permuted y via GCV. If FALSE (default), use the eta from the original fit.
#'   Setting FALSE is faster and usually sufficient for null characterization.
#' @param seed Random seed for reproducibility
#' @param verbose Logical. Print progress (default TRUE)
#' @param n.cores Number of cores for parallel processing (default 1)
#'
#' @return A list of class "lslope.null" containing:
#'   \describe{
#'     \item{null.values}{Matrix (n.perm * p) x n of null lslope values if
#'       pool.features = TRUE, otherwise list of p matrices each n.perm x n}
#'     \item{null.params}{List with vertex-wise null distribution parameters:
#'       mean, sd, quantiles}
#'     \item{n.perm}{Number of permutations used}
#'     \item{n.features}{Number of features}
#'     \item{n.vertices}{Number of vertices}
#'     \item{pooled}{Whether features were pooled}
#'     \item{lslope.type}{Type of lslope measure used}
#'   }
#'
#' @details
#' The function permutes the ORIGINAL response vector y, then applies spectral
#' smoothing to obtain y.hat.perm. This is crucial because:
#'
#' 1. Permuting y.hat directly would destroy the geometric structure imposed
#'    by smoothing, creating an invalid null distribution.
#'
#' 2. Under the null hypothesis (no association between y and Z), permuting y
#'    preserves the marginal distribution while breaking any true association.
#'
#' 3. Smoothing the permuted y ensures the null lslope values have the same
#'    geometric properties as the observed values.
#'
#' The smoothing of permuted y is efficient because we reuse the spectral
#' decomposition (eigenvectors V and filtered eigenvalues) from the original
#' fit. Each permutation requires only O(nm) operations for the matrix
#' multiplications V * diag(f_lambda) * V^T * y_perm.
#'
#' When \code{pool.features = TRUE} (recommended), null values from all features
#' are pooled to estimate a single vertex-wise null distribution. This is
#' appropriate when features are exchangeable under the null hypothesis.
#'
#' @examples
#' \dontrun{
#' ## Fit model
#' fit <- fit.rdgraph.regression(X, y, k = 15)
#'
#' ## Smooth features
#' Z.hat <- refit.rdgraph.regression(fit, Z, per.column.gcv = TRUE)
#'
#' ## Generate null distribution (permuting original y, not y.hat!)
#' null.dist <- lslope.null.distribution(
#'     fitted.model = fit,
#'     y = y,  # Original response, not fit$fitted.values
#'     Z.hat = Z.hat$fitted.values,
#'     n.perm = 200
#' )
#' }
#'
#' @export
lslope.null.distribution <- function(fitted.model,
                                     y,
                                     Z.hat,
                                     n.perm = 200L,
                                     lslope.type = c("normalized", "slope", "sign"),
                                     ascending = TRUE,
                                     pool.features = TRUE,
                                     per.column.gcv = FALSE,
                                     seed = 12345L,
                                     verbose = TRUE,
                                     n.cores = 1L) {

    lslope.type <- match.arg(lslope.type)

    ## Validate inputs
    if (!inherits(fitted.model, "knn.riem.fit")) {
        stop("fitted.model must be a 'knn.riem.fit' object from fit.rdgraph.regression()")
    }

    if (is.null(fitted.model$spectral)) {
        stop("fitted.model must contain spectral decomposition for efficient re-smoothing")
    }

    n <- length(y)

    if (is.vector(Z.hat)) {
        Z.hat <- matrix(Z.hat, ncol = 1)
    }
    if (nrow(Z.hat) != n) {
        stop("nrow(Z.hat) must equal length(y)")
    }
    p <- ncol(Z.hat)

    ## Extract graph components
    adj.list <- fitted.model$graph$adj.list
    weight.list <- fitted.model$graph$edge.length.list

    if (verbose) {
        message(sprintf("Generating null distribution:"))
        message(sprintf("  Permutations: %d", n.perm))
        message(sprintf("  Features: %d, Vertices: %d", p, n))
        message(sprintf("  Method: Permute y -> smooth -> compute lslope"))
        if (per.column.gcv) {
            message("  Re-selecting eta via GCV for each permutation")
        } else {
            message("  Using fixed eta from original fit")
        }
    }

    set.seed(seed)

    ## Storage for null values
    if (pool.features) {
        null.values <- matrix(NA_real_, nrow = n.perm * p, ncol = n)
    } else {
        null.values <- vector("list", p)
        for (j in seq_len(p)) {
            null.values[[j]] <- matrix(NA_real_, nrow = n.perm, ncol = n)
        }
    }

    ## Check for parallel processing
    use.parallel <- (n.cores > 1L) && requireNamespace("foreach", quietly = TRUE) &&
                    requireNamespace("doParallel", quietly = TRUE)

    if (use.parallel) {

        ## ====================================================================
        ## PARALLEL IMPLEMENTATION
        ## ====================================================================

        max.cores <- parallel::detectCores(logical = FALSE)
        n.cores.use <- min(n.cores, max.cores, n.perm)

        if (verbose) {
            message(sprintf("  Using %d cores for parallel processing", n.cores.use))
        }

        cl <- parallel::makeCluster(n.cores.use)
        doParallel::registerDoParallel(cl)
        on.exit(parallel::stopCluster(cl), add = TRUE)

        b <- NULL  # Avoid R CMD check NOTE

        results.list <- foreach::foreach(
            b = seq_len(n.perm),
            .combine = "c",
            .packages = "gflow"
        ) %dopar% {

            ## Permute ORIGINAL y
            y.perm <- y[sample.int(n)]

            ## Smooth the permuted y using the fitted model's spectral structure
            y.hat.perm.result <- refit.rdgraph.regression(
                fitted.model,
                y.perm,
                per.column.gcv = per.column.gcv,
                verbose = FALSE
            )
            y.hat.perm <- y.hat.perm.result$fitted.values

            ## Compute lslope for all features against smoothed permuted y
            lslope.perm <- matrix(NA_real_, nrow = p, ncol = n)

            for (j in seq_len(p)) {
                res <- lslope.gradient(
                    adj.list, weight.list,
                    y.hat.perm, Z.hat[, j],
                    type = lslope.type,
                    ascending = ascending
                )
                lslope.perm[j, ] <- res$coefficients
            }

            ## Return as list element
            list(list(b = b, lslope = lslope.perm))
        }

        ## Reassemble results
        for (item in results.list) {
            b <- item$b
            lslope.perm <- item$lslope

            if (pool.features) {
                row.start <- (b - 1) * p + 1
                row.end <- b * p
                null.values[row.start:row.end, ] <- lslope.perm
            } else {
                for (j in seq_len(p)) {
                    null.values[[j]][b, ] <- lslope.perm[j, ]
                }
            }
        }

    } else {

        ## ====================================================================
        ## SEQUENTIAL IMPLEMENTATION
        ## ====================================================================

        if (verbose) {
            pb <- txtProgressBar(min = 0, max = n.perm, style = 3)
        }

        for (b in seq_len(n.perm)) {

            ## Permute ORIGINAL y (not y.hat!)
            y.perm <- y[sample.int(n)]

            ## Smooth the permuted y using the fitted model's spectral structure
            ## This is the key step: we apply the same smoothing to permuted y
            y.hat.perm.result <- refit.rdgraph.regression(
                fitted.model,
                y.perm,
                per.column.gcv = per.column.gcv,
                verbose = FALSE
            )
            y.hat.perm <- y.hat.perm.result$fitted.values

            ## Compute lslope for all features against smoothed permuted y
            for (j in seq_len(p)) {
                res <- lslope.gradient(
                    adj.list, weight.list,
                    y.hat.perm, Z.hat[, j],
                    type = lslope.type,
                    ascending = ascending
                )

                if (pool.features) {
                    row.idx <- (b - 1) * p + j
                    null.values[row.idx, ] <- res$coefficients
                } else {
                    null.values[[j]][b, ] <- res$coefficients
                }
            }

            if (verbose) {
                setTxtProgressBar(pb, b)
            }
        }

        if (verbose) {
            close(pb)
        }
    }

    ## Compute null distribution parameters at each vertex
    if (pool.features) {
        null.params <- list(
            mean = apply(null.values, 2, mean, na.rm = TRUE),
            sd = apply(null.values, 2, sd, na.rm = TRUE),
            median = apply(null.values, 2, median, na.rm = TRUE),
            mad = apply(null.values, 2, mad, na.rm = TRUE),
            quantiles = apply(null.values, 2, function(x) {
                quantile(x, probs = c(0.005, 0.025, 0.05, 0.95, 0.975, 0.995),
                         na.rm = TRUE)
            })
        )
    } else {
        null.params <- vector("list", p)
        for (j in seq_len(p)) {
            null.params[[j]] <- list(
                mean = apply(null.values[[j]], 2, mean, na.rm = TRUE),
                sd = apply(null.values[[j]], 2, sd, na.rm = TRUE),
                median = apply(null.values[[j]], 2, median, na.rm = TRUE),
                mad = apply(null.values[[j]], 2, mad, na.rm = TRUE)
            )
        }
    }

    result <- list(
        null.values = null.values,
        null.params = null.params,
        n.perm = n.perm,
        n.features = p,
        n.vertices = n,
        pooled = pool.features,
        lslope.type = lslope.type,
        ascending = ascending,
        per.column.gcv = per.column.gcv
    )

    class(result) <- c("lslope.null", "list")
    return(result)
}


#' @export
print.lslope.null <- function(x, ...) {
    cat("\nNull Distribution for lslope()\n")
    cat("==============================\n\n")
    cat(sprintf("Permutations: %d\n", x$n.perm))
    cat(sprintf("Features: %d\n", x$n.features))
    cat(sprintf("Vertices: %d\n", x$n.vertices))
    cat(sprintf("Pooled: %s\n", x$pooled))
    cat(sprintf("lslope type: %s\n", x$lslope.type))
    cat(sprintf("Per-permutation GCV: %s\n", x$per.column.gcv))

    if (x$pooled) {
        cat(sprintf("\nNull samples per vertex: %d\n", x$n.perm * x$n.features))
        cat(sprintf("Mean null value (median across vertices): %.4f\n",
                    median(x$null.params$mean)))
        cat(sprintf("SD of null (median across vertices): %.4f\n",
                    median(x$null.params$sd)))
    }

    invisible(x)
}


## ============================================================================
## STEP 2: COMPUTE OBSERVED lslope VALUES
## ============================================================================

#' Compute Observed lslope Values for All Features
#'
#' Computes the observed lslope() values between the smoothed response y.hat
#' and each column of the smoothed feature matrix Z.hat.
#'
#' @param adj.list Adjacency list (1-based R indexing)
#' @param weight.list Edge weight list
#' @param y.hat Smoothed response values (length n)
#' @param Z.hat Matrix of smoothed feature values (n x p)
#' @param lslope.type Type of lslope measure
#' @param ascending Logical. Use ascending gradient direction
#' @param verbose Logical. Print progress
#'
#' @return Matrix (p x n) of observed lslope values, with features in rows
#'   and vertices in columns
#'
#' @export
lslope.observed <- function(adj.list,
                             weight.list,
                             y.hat,
                             Z.hat,
                             lslope.type = c("normalized", "slope", "sign"),
                             ascending = TRUE,
                             verbose = FALSE) {

    lslope.type <- match.arg(lslope.type)

    n <- length(y.hat)
    if (is.vector(Z.hat)) {
        Z.hat <- matrix(Z.hat, ncol = 1)
    }
    p <- ncol(Z.hat)

    ## Storage: features in rows, vertices in columns
    obs.lslope <- matrix(NA_real_, nrow = p, ncol = n)
    rownames(obs.lslope) <- colnames(Z.hat)

    if (verbose && p > 10) {
        pb <- txtProgressBar(min = 0, max = p, style = 3)
    }

    for (j in seq_len(p)) {
        res <- lslope.gradient(
            adj.list, weight.list,
            y.hat, Z.hat[, j],
            type = lslope.type,
            ascending = ascending
        )
        obs.lslope[j, ] <- res$coefficients

        if (verbose && p > 10) {
            setTxtProgressBar(pb, j)
        }
    }

    if (verbose && p > 10) {
        close(pb)
    }

    return(obs.lslope)
}


## ============================================================================
## STEP 3: FIT TWO-COMPONENT MIXTURE MODEL
## ============================================================================

#' Fit Two-Component Mixture Model for lslope Values
#'
#' Fits an empirical Bayes two-component mixture model to distinguish signal
#' (true associations) from noise (null) using the permutation-derived null
#' distribution.
#'
#' The model assumes each observed lslope value comes from either:
#' - Null component f0: No true association (characterized by permutation)
#' - Alternative component f1: True association (estimated from data)
#'
#' The mixing proportion pi0 (proportion of nulls) is estimated from the data
#' using the convex optimization approach or method of moments.
#'
#' @param obs.lslope Matrix (p x n) of observed lslope values from
#'   \code{lslope.observed()}
#' @param null.dist Null distribution object from \code{lslope.null.distribution()}
#' @param pi0.method Method for estimating pi0: "convex" (default), "quantile",
#'   or "fixed"
#' @param pi0.fixed Fixed value of pi0 when pi0.method = "fixed"
#' @param null.model Parametric model for null: "normal" or "empirical"
#' @param alt.model Parametric model for alternative: "normal" or "empirical"
#' @param verbose Logical. Print fitting information
#'
#' @return A list of class "lslope.mixture" containing:
#'   \describe{
#'     \item{pi0}{Estimated or fixed proportion of nulls (scalar or vector)}
#'     \item{null.density}{Function or list for evaluating null density}
#'     \item{alt.params}{Estimated alternative distribution parameters}
#'     \item{prob.null}{Matrix (p x n) of P(null | observed lslope)}
#'     \item{lfdr}{Matrix (p x n) of local false discovery rates}
#'     \item{obs.lslope}{Copy of input observed values}
#'   }
#'
#' @details
#' The local false discovery rate (lfdr) at each vertex for each feature is:
#' \deqn{lfdr(c_{jv}) = P(null | c_{jv}) = \frac{\pi_0 f_0(c_{jv})}{\pi_0 f_0(c_{jv}) + (1-\pi_0) f_1(c_{jv})}}
#'
#' where \eqn{c_{jv}} is the observed lslope value for the \eqn{j}-th feature
#' and vertex \eqn{v}, f0 is the null density, f1 is the alternative density,
#' and pi0 is the mixing proportion.
#'
#' @export
fit.lslope.mixture <- function(obs.lslope,
                                null.dist,
                                pi0.method = c("convex", "quantile", "fixed"),
                                pi0.fixed = 0.9,
                                null.model = c("normal", "empirical"),
                                alt.model = c("normal", "empirical"),
                                verbose = TRUE) {

    pi0.method <- match.arg(pi0.method)
    null.model <- match.arg(null.model)
    alt.model <- match.arg(alt.model)

    if (!inherits(null.dist, "lslope.null")) {
        stop("null.dist must be a 'lslope.null' object from lslope.null.distribution()")
    }

    p <- nrow(obs.lslope)
    n <- ncol(obs.lslope)

    if (n != null.dist$n.vertices) {
        stop("Number of vertices in obs.lslope must match null.dist")
    }

    if (verbose) {
        message(sprintf("Fitting mixture model: %d features, %d vertices", p, n))
        message(sprintf("Null model: %s, Alternative model: %s", null.model, alt.model))
    }

    ## ========================================================================
    ## Build null density function
    ## ========================================================================

    if (null.model == "normal") {
        ## Parametric normal null using permutation-estimated mean and SD
        null.mean <- null.dist$null.params$mean
        null.sd <- null.dist$null.params$sd

        ## Ensure positive SD
        null.sd <- pmax(null.sd, 1e-10)

        null.density <- function(x, v) {
            dnorm(x, mean = null.mean[v], sd = null.sd[v])
        }
    } else {
        ## Empirical null using kernel density estimation
        null.density <- function(x, v) {
            null.vals <- null.dist$null.values[, v]
            null.vals <- null.vals[is.finite(null.vals)]
            if (length(null.vals) < 10) {
                return(1e-10)
            }
            bw <- bw.nrd0(null.vals)
            sum(dnorm(x, mean = null.vals, sd = bw)) / length(null.vals)
        }
    }

    ## ========================================================================
    ## Estimate pi0 (proportion of nulls)
    ## ========================================================================

    if (pi0.method == "fixed") {
        pi0 <- pi0.fixed
        if (verbose) {
            message(sprintf("Using fixed pi0 = %.3f", pi0))
        }
    } else if (pi0.method == "quantile") {
        ## Storey's quantile method
        pi0.by.vertex <- numeric(n)

        for (v in seq_len(n)) {
            obs.v <- obs.lslope[, v]
            null.mean.v <- null.dist$null.params$mean[v]
            null.sd.v <- null.dist$null.params$sd[v]

            in.null <- abs(obs.v - null.mean.v) < null.sd.v
            pi0.by.vertex[v] <- mean(in.null, na.rm = TRUE)
        }

        pi0 <- median(pi0.by.vertex, na.rm = TRUE)
        pi0 <- min(max(pi0, 0.1), 0.99)

        if (verbose) {
            message(sprintf("Estimated pi0 = %.3f (quantile method)", pi0))
        }

    } else {
        ## Convex optimization method
        all.obs <- as.vector(obs.lslope)
        all.obs <- all.obs[is.finite(all.obs)]

        null.mean.global <- median(null.dist$null.params$mean)
        null.sd.global <- median(null.dist$null.params$sd)

        central <- abs(all.obs - null.mean.global) < 0.5 * null.sd.global
        pi0 <- min(1, 2 * mean(central))
        pi0 <- min(max(pi0, 0.1), 0.99)

        if (verbose) {
            message(sprintf("Estimated pi0 = %.3f (convex method)", pi0))
        }
    }

    ## ========================================================================
    ## Estimate alternative distribution
    ## ========================================================================

    if (alt.model == "normal") {
        alt.params <- list(
            mean = matrix(NA_real_, nrow = p, ncol = n),
            sd = matrix(NA_real_, nrow = p, ncol = n)
        )

        for (v in seq_len(n)) {
            null.mean.v <- null.dist$null.params$mean[v]
            null.sd.v <- null.dist$null.params$sd[v]

            obs.v <- obs.lslope[, v]
            z.scores <- (obs.v - null.mean.v) / null.sd.v
            is.tail <- abs(z.scores) > 1.5

            if (sum(is.tail, na.rm = TRUE) >= 5) {
                tail.obs <- obs.v[is.tail]
                alt.params$mean[, v] <- mean(tail.obs, na.rm = TRUE)
                alt.params$sd[, v] <- sd(tail.obs, na.rm = TRUE)
            } else {
                alt.params$mean[, v] <- mean(obs.v, na.rm = TRUE)
                alt.params$sd[, v] <- sd(obs.v, na.rm = TRUE)
            }
        }

        alt.params$sd <- pmax(alt.params$sd, 1e-10)

        alt.density <- function(x, v, j = NULL) {
            if (is.null(j)) {
                mean.v <- mean(alt.params$mean[, v], na.rm = TRUE)
                sd.v <- mean(alt.params$sd[, v], na.rm = TRUE)
            } else {
                mean.v <- alt.params$mean[j, v]
                sd.v <- alt.params$sd[j, v]
            }
            dnorm(x, mean = mean.v, sd = sd.v)
        }

    } else {
        alt.params <- NULL

        alt.density <- function(x, v, j = NULL) {
            obs.v <- obs.lslope[, v]
            null.mean.v <- null.dist$null.params$mean[v]
            null.sd.v <- null.dist$null.params$sd[v]

            z.scores <- (obs.v - null.mean.v) / null.sd.v
            tail.obs <- obs.v[abs(z.scores) > 1.5]

            if (length(tail.obs) < 5) {
                tail.obs <- obs.v
            }
            tail.obs <- tail.obs[is.finite(tail.obs)]

            if (length(tail.obs) < 3) {
                return(1e-10)
            }

            bw <- bw.nrd0(tail.obs)
            sum(dnorm(x, mean = tail.obs, sd = bw)) / length(tail.obs)
        }
    }

    ## ========================================================================
    ## Compute posterior probabilities
    ## ========================================================================

    prob.null <- matrix(NA_real_, nrow = p, ncol = n)
    rownames(prob.null) <- rownames(obs.lslope)

    if (verbose) {
        message("Computing posterior probabilities...")
    }

    for (v in seq_len(n)) {
        for (j in seq_len(p)) {
            c.jv <- obs.lslope[j, v]

            if (!is.finite(c.jv)) {
                prob.null[j, v] <- NA_real_
                next
            }

            f0 <- null.density(c.jv, v)
            f1 <- alt.density(c.jv, v, j)
            f.mix <- pi0 * f0 + (1 - pi0) * f1

            if (f.mix > 1e-15) {
                prob.null[j, v] <- pi0 * f0 / f.mix
            } else {
                prob.null[j, v] <- pi0
            }
        }
    }

    prob.null <- pmin(pmax(prob.null, 1e-10), 1 - 1e-10)
    lfdr <- prob.null

    result <- list(
        pi0 = pi0,
        null.density = null.density,
        null.params = null.dist$null.params,
        alt.density = alt.density,
        alt.params = alt.params,
        prob.null = prob.null,
        lfdr = lfdr,
        obs.lslope = obs.lslope,
        n.features = p,
        n.vertices = n,
        null.model = null.model,
        alt.model = alt.model,
        pi0.method = pi0.method
    )

    class(result) <- c("lslope.mixture", "list")
    return(result)
}


#' @export
print.lslope.mixture <- function(x, ...) {
    cat("\nTwo-Component Mixture Model for lslope()\n")
    cat("========================================\n\n")

    cat(sprintf("Features: %d\n", x$n.features))
    cat(sprintf("Vertices: %d\n", x$n.vertices))
    cat(sprintf("pi0 (null proportion): %.3f\n", x$pi0))
    cat(sprintf("Null model: %s\n", x$null.model))
    cat(sprintf("Alternative model: %s\n", x$alt.model))

    prob.alt <- 1 - x$prob.null
    cat(sprintf("\nP(alternative) summary:\n"))
    cat(sprintf("  Mean: %.3f\n", mean(prob.alt, na.rm = TRUE)))
    cat(sprintf("  Median: %.3f\n", median(prob.alt, na.rm = TRUE)))

    n.sig.50 <- sum(prob.alt > 0.5, na.rm = TRUE)
    n.sig.90 <- sum(prob.alt > 0.9, na.rm = TRUE)
    n.sig.95 <- sum(prob.alt > 0.95, na.rm = TRUE)

    cat(sprintf("\nSignificant associations (feature-vertex pairs):\n"))
    cat(sprintf("  P(alt) > 0.50: %d (%.1f%%)\n",
                n.sig.50, 100 * n.sig.50 / length(prob.alt)))
    cat(sprintf("  P(alt) > 0.90: %d (%.1f%%)\n",
                n.sig.90, 100 * n.sig.90 / length(prob.alt)))
    cat(sprintf("  P(alt) > 0.95: %d (%.1f%%)\n",
                n.sig.95, 100 * n.sig.95 / length(prob.alt)))

    invisible(x)
}


#' @export
summary.lslope.mixture <- function(object, threshold = 0.9, ...) {

    prob.alt <- 1 - object$prob.null
    p <- object$n.features
    n <- object$n.vertices

    feature.summary <- data.frame(
        feature = rownames(object$obs.lslope),
        n.sig = apply(prob.alt > threshold, 1, sum, na.rm = TRUE),
        prop.sig = apply(prob.alt > threshold, 1, mean, na.rm = TRUE),
        mean.prob.alt = apply(prob.alt, 1, mean, na.rm = TRUE),
        max.prob.alt = apply(prob.alt, 1, max, na.rm = TRUE),
        mean.lslope = apply(object$obs.lslope, 1, mean, na.rm = TRUE),
        stringsAsFactors = FALSE
    )

    feature.summary <- feature.summary[order(-feature.summary$n.sig), ]

    vertex.summary <- data.frame(
        vertex = seq_len(n),
        n.sig.features = apply(prob.alt > threshold, 2, sum, na.rm = TRUE),
        mean.prob.alt = apply(prob.alt, 2, mean, na.rm = TRUE)
    )

    result <- list(
        feature.summary = feature.summary,
        vertex.summary = vertex.summary,
        threshold = threshold,
        pi0 = object$pi0
    )

    class(result) <- "summary.lslope.mixture"
    return(result)
}


#' @export
print.summary.lslope.mixture <- function(x, n.top = 20, ...) {
    cat("\nSummary: lslope Mixture Model\n")
    cat("=============================\n\n")

    cat(sprintf("Significance threshold: P(alt) > %.2f\n", x$threshold))
    cat(sprintf("Estimated pi0: %.3f\n\n", x$pi0))

    cat("Top features by number of significant vertices:\n\n")

    top.features <- head(x$feature.summary, n.top)
    print(top.features, row.names = FALSE)

    invisible(x)
}


## ============================================================================
## STEP 4: AGGREGATE BY BASINS
## ============================================================================

#' Aggregate Mixture Model Results by Basins
#'
#' Computes basin-level summaries of the posterior probabilities, identifying
#' context-dependent associations where features show different behavior in
#' different basins.
#'
#' @param mixture.result Result from \code{fit.lslope.mixture()}
#' @param basin.membership Integer vector of basin assignments (length n)
#' @param threshold Probability threshold for "significant" association
#'
#' @return A list containing:
#'   \describe{
#'     \item{basin.feature.probs}{Matrix (n.basins x p) of mean P(alt) by basin}
#'     \item{basin.stats}{Data frame with per-basin, per-feature statistics}
#'     \item{context.features}{Features showing context-dependent associations}
#'   }
#'
#' @export
aggregate.by.basin <- function(mixture.result,
                                basin.membership,
                                threshold = 0.9) {

    if (!inherits(mixture.result, "lslope.mixture")) {
        stop("mixture.result must be a 'lslope.mixture' object")
    }

    prob.alt <- 1 - mixture.result$prob.null
    p <- mixture.result$n.features
    n <- mixture.result$n.vertices

    if (length(basin.membership) != n) {
        stop("basin.membership length must equal number of vertices")
    }

    basins <- sort(unique(basin.membership))
    n.basins <- length(basins)
    feature.names <- rownames(mixture.result$obs.lslope)
    if (is.null(feature.names)) {
        feature.names <- paste0("Feature", seq_len(p))
    }

    basin.feature.probs <- matrix(NA_real_, nrow = n.basins, ncol = p)
    rownames(basin.feature.probs) <- paste0("Basin", basins)
    colnames(basin.feature.probs) <- feature.names

    basin.stats <- data.frame(
        basin = integer(),
        feature = character(),
        n.vertices = integer(),
        mean.prob.alt = numeric(),
        median.prob.alt = numeric(),
        prop.sig = numeric(),
        mean.lslope = numeric(),
        stringsAsFactors = FALSE
    )

    for (k in seq_along(basins)) {
        basin.k <- basins[k]
        in.basin <- which(basin.membership == basin.k)
        n.in.basin <- length(in.basin)

        for (j in seq_len(p)) {
            prob.alt.jk <- prob.alt[j, in.basin]
            lslope.jk <- mixture.result$obs.lslope[j, in.basin]

            mean.prob <- mean(prob.alt.jk, na.rm = TRUE)
            basin.feature.probs[k, j] <- mean.prob

            basin.stats <- rbind(basin.stats, data.frame(
                basin = basin.k,
                feature = feature.names[j],
                n.vertices = n.in.basin,
                mean.prob.alt = mean.prob,
                median.prob.alt = median(prob.alt.jk, na.rm = TRUE),
                prop.sig = mean(prob.alt.jk > threshold, na.rm = TRUE),
                mean.lslope = mean(lslope.jk, na.rm = TRUE),
                stringsAsFactors = FALSE
            ))
        }
    }

    feature.heterogeneity <- apply(basin.feature.probs, 2, sd, na.rm = TRUE)
    context.features <- feature.names[order(-feature.heterogeneity)]

    result <- list(
        basin.feature.probs = basin.feature.probs,
        basin.stats = basin.stats,
        context.features = context.features,
        feature.heterogeneity = feature.heterogeneity[order(-feature.heterogeneity)],
        n.basins = n.basins,
        threshold = threshold
    )

    class(result) <- c("lslope.basin.summary", "list")
    return(result)
}


#' @export
print.lslope.basin.summary <- function(x, n.top = 10, ...) {
    cat("\nBasin-Level lslope Association Summary\n")
    cat("======================================\n\n")

    cat(sprintf("Basins: %d\n", x$n.basins))
    cat(sprintf("Features: %d\n", ncol(x$basin.feature.probs)))
    cat(sprintf("Threshold: P(alt) > %.2f\n\n", x$threshold))

    cat("Top context-dependent features (high heterogeneity across basins):\n\n")

    top.features <- head(x$context.features, n.top)
    top.het <- head(x$feature.heterogeneity, n.top)

    for (i in seq_along(top.features)) {
        cat(sprintf("  %2d. %s (SD = %.3f)\n", i, top.features[i], top.het[i]))
    }

    invisible(x)
}


## ============================================================================
## CONVENIENCE WRAPPER
## ============================================================================

#' Full Bayesian lslope Association Testing Pipeline
#'
#' Convenience function that runs the complete pipeline:
#' 1. Smooth features using the fitted model
#' 2. Generate null distribution via permutation of original y
#' 3. Compute observed lslope values
#' 4. Fit two-component mixture model
#' 5. Optionally aggregate by basins
#'
#' @param fitted.model Fitted model from \code{fit.rdgraph.regression()}
#' @param y Original (unsmoothed) response vector. This is permuted to
#'   generate the null distribution.
#' @param Z Matrix of features to test (n x p), unsmoothed
#' @param basin.membership Optional basin assignments for aggregation
#' @param n.perm Number of permutations for null (default 200)
#' @param lslope.type Type of lslope measure (default "normalized")
#' @param per.column.gcv.features Logical. Use per-column GCV when smoothing
#'   features Z (default TRUE, recommended for heterogeneous features)
#' @param per.column.gcv.null Logical. Use per-column GCV when smoothing
#'   permuted y for null distribution (default FALSE, faster)
#' @param pi0.method Method for estimating pi0
#' @param threshold Significance threshold for basin aggregation
#' @param seed Random seed
#' @param verbose Print progress
#' @param n.cores Number of cores for parallel processing
#'
#' @return A comprehensive result object with all pipeline outputs
#'
#' @examples
#' \dontrun{
#' ## Fit model on response
#' fit <- fit.rdgraph.regression(X, y, k = 15)
#'
#' ## Run full pipeline (note: pass original y, not fitted.values!)
#' result <- lslope.association.test(
#'     fitted.model = fit,
#'     y = y,  # Original response
#'     Z = Z,  # Feature matrix
#'     basin.membership = basins$membership,
#'     n.perm = 200,
#'     n.cores = 4
#' )
#'
#' ## View results
#' print(result$mixture)
#' print(result$basin.summary)
#' }
#'
#' @export
lslope.association.test <- function(fitted.model,
                                     y,
                                     Z,
                                     basin.membership = NULL,
                                     n.perm = 200L,
                                     lslope.type = "normalized",
                                     per.column.gcv.features = TRUE,
                                     per.column.gcv.null = FALSE,
                                     pi0.method = "convex",
                                     threshold = 0.9,
                                     seed = 12345L,
                                     verbose = TRUE,
                                     n.cores = 1L) {

    if (!inherits(fitted.model, "knn.riem.fit")) {
        stop("fitted.model must be from fit.rdgraph.regression()")
    }

    n <- length(y)
    if (is.vector(Z)) {
        Z <- matrix(Z, ncol = 1)
    }
    if (nrow(Z) != n) {
        stop("nrow(Z) must equal length(y)")
    }

    adj.list <- fitted.model$graph$adj.list
    weight.list <- fitted.model$graph$edge.length.list
    y.hat <- fitted.model$fitted.values

    ## Step 1: Smooth features
    if (verbose) message("\n=== Step 1: Smoothing features ===\n")

    Z.hat.result <- refit.rdgraph.regression(
        fitted.model, Z,
        per.column.gcv = per.column.gcv.features,
        verbose = verbose
    )
    Z.hat <- Z.hat.result$fitted.values

    ## Step 2: Generate null distribution (permuting original y!)
    if (verbose) message("\n=== Step 2: Generating null distribution ===\n")

    null.dist <- lslope.null.distribution(
        fitted.model = fitted.model,
        y = y,  # Original y, not y.hat!
        Z.hat = Z.hat,
        n.perm = n.perm,
        lslope.type = lslope.type,
        per.column.gcv = per.column.gcv.null,
        seed = seed,
        verbose = verbose,
        n.cores = n.cores
    )

    ## Step 3: Compute observed lslope values
    if (verbose) message("\n=== Step 3: Computing observed lslope values ===\n")

    obs.lslope <- lslope.observed(
        adj.list, weight.list,
        y.hat, Z.hat,
        lslope.type = lslope.type,
        verbose = verbose
    )

    ## Step 4: Fit mixture model
    if (verbose) message("\n=== Step 4: Fitting mixture model ===\n")

    mixture <- fit.lslope.mixture(
        obs.lslope, null.dist,
        pi0.method = pi0.method,
        verbose = verbose
    )

    ## Step 5: Aggregate by basins (if provided)
    basin.summary <- NULL
    if (!is.null(basin.membership)) {
        if (verbose) message("\n=== Step 5: Aggregating by basins ===\n")

        basin.summary <- aggregate.by.basin(
            mixture, basin.membership,
            threshold = threshold
        )
    }

    result <- list(
        null.dist = null.dist,
        obs.lslope = obs.lslope,
        mixture = mixture,
        basin.summary = basin.summary,
        Z.hat = Z.hat,
        parameters = list(
            n.perm = n.perm,
            lslope.type = lslope.type,
            per.column.gcv.features = per.column.gcv.features,
            per.column.gcv.null = per.column.gcv.null,
            pi0.method = pi0.method,
            threshold = threshold,
            seed = seed
        )
    )

    class(result) <- c("lslope.test.result", "list")
    return(result)
}


#' @export
print.lslope.test.result <- function(x, ...) {
    cat("\n============================================\n")
    cat("Bayesian lslope Association Test Results\n")
    cat("============================================\n")

    print(x$mixture)

    if (!is.null(x$basin.summary)) {
        cat("\n")
        print(x$basin.summary)
    }

    invisible(x)
}
