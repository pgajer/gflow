---
title: "Noisy Circle: End-to-End gflow Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Noisy Circle: End-to-End gflow Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5,
  message = FALSE,
  warning = FALSE
)
library(gflow)
set.seed(2026)
```

## Overview

This vignette follows one complete analysis story.
We start with noisy samples from a circle and a noisy response field over that
circle. We then construct graph scales, denoise geometry, smooth responses and
features, recover local extrema/basins, and finally test local associations
with permutation-based inference.

## 1) Simulate Geometry and Response

```{r simulate-data}
n <- 250L
radius <- 1

X.df <- generate.circle.data(
  n = n,
  radius = radius,
  noise = 0.15,
  type = "random",
  noise.type = "normal",
  seed = 11
)
X <- as.matrix(X.df[, c("x", "y")])

if ("angles" %in% colnames(X.df)) {
  theta.obs <- as.numeric(X.df$angles)
} else {
  theta.obs <- atan2(X[, 2], X[, 1])
  theta.obs <- ifelse(theta.obs < 0, theta.obs + 2 * pi, theta.obs)
}

# Response: a mixture of two circular Gaussian peaks (one larger, one smaller)
mu1 <- 0.80
mu2 <- 3.95
sd1 <- 0.30
sd2 <- 0.48
amp1 <- 1.40
amp2 <- 0.75

comp1 <- circular.synthetic.mixture.of.gaussians(
  x = theta.obs,
  x.knot = mu1,
  y.knot = amp1,
  sd.knot = sd1
)
comp2 <- circular.synthetic.mixture.of.gaussians(
  x = theta.obs,
  x.knot = mu2,
  y.knot = amp2,
  sd.knot = sd2
)
y.true <- comp1 + comp2
y <- y.true + rnorm(n, sd = 0.10)

circle.true.df <- generate.circle.data(
  n = 300,
  radius = radius,
  noise = 0,
  type = "uniform",
  noise.type = "normal",
  seed = 1
)
circle.true <- as.matrix(circle.true.df[, c("x", "y")])
```

```{r noisy-circle-plot}
op <- par(mar = c(2.5, 2.5, 0.5, 0.5), mgp = c(2.5, 0.5, 0), tcl = -0.3)
plot(X, asp = 1, las = 1, xlab = "", ylab = "")
lines(circle.true, lwd = 2)
legend("topleft",
       legend = c("Observed samples", "True circle"),
       pch = c(1, NA),
       lty = c(NA, 1),
       col = c("black", "black"),
       bty = "n", cex = 0.9)
par(op)
```

## 2) Build Graphs Across k

```{r iknn-graphs}
k.min <- 5L
k.max <- 12L

X.graphs <- create.iknn.graphs(
  X,
  kmin = k.min,
  kmax = k.max,
  max.path.edge.ratio.deviation.thld = 0.1,
  n.cores = 1L,
  verbose = TRUE
)
summary(X.graphs)
```

## 3) Select Graph Scale

```{r iknn-selectk}
X.graphs2 <- build.iknn.graphs.and.selectk(
  X,
  kmin = k.min,
  kmax = k.max,
  method = "edit",
  n.cores = 1L,
  verbose = TRUE
)

X.graphs2$k.opt.edit
plot(X.graphs2)
```

## 4) Visualize the Selected Graph

```{r graph-layout-2d}
k.values <- if (!is.null(colnames(X.graphs$k_statistics)) &&
                "k" %in% colnames(X.graphs$k_statistics)) {
  as.integer(X.graphs$k_statistics[, "k"])
} else {
  seq_len(length(X.graphs$geom_pruned_graphs))
}

sel.k <- X.graphs2$k.opt.edit
if (is.null(sel.k) || is.na(sel.k)) {
  sel.k <- k.values[1]
}
sel.k <- as.integer(sel.k)
k.idx <- which(k.values == sel.k)[1]
if (is.na(k.idx)) k.idx <- 1L

g.sel <- X.graphs$geom_pruned_graphs[[k.idx]]
layout.2d <- graph.embedding(
  adj.list = g.sel$adj_list,
  weights.list = g.sel$weight_list,
  invert.weights = TRUE,
  dim = 2,
  method = "fr"
)

plot(layout.2d, pch = 19, cex = 0.5, asp = 1,
     main = sprintf("2D graph embedding for selected k = %d", sel.k),
     xlab = "", ylab = "")
```

## 5) Denoise Geometry with `data.smoother()`

```{r data-smoother}
X.denoise.res <- data.smoother(
  X,
  kmin = k.min,
  kmax = k.max,
  n.cores = 1L,
  proxy.response = "pc1",
  max.iterations = 8L,
  n.eigenpairs = 100L,
  filter.type = "heat_kernel",
  t.scale.factor = 0.5,
  beta.coef.factor = 0.1,
  verbose = TRUE
)

X.smoothed <- X.denoise.res$X.smoothed
X.denoise.res$k.best
```

```{r align-trimmed-data}
if (isTRUE(X.denoise.res$trimmed)) {
  kept.rows <- X.denoise.res$kept.rows
  X.use <- X[kept.rows, , drop = FALSE]
  theta.use <- theta.obs[kept.rows]
  y.use <- y[kept.rows]
  y.true.use <- y.true[kept.rows]
  comp1.use <- comp1[kept.rows]
  comp2.use <- comp2[kept.rows]
} else {
  X.use <- X
  theta.use <- theta.obs
  y.use <- y
  y.true.use <- y.true
  comp1.use <- comp1
  comp2.use <- comp2
}
```

```{r denoising-panels, fig.width=10, fig.height=4}
op <- par(mfrow = c(1, 2),
          mar = c(2.75, 2.75, 0.5, 0.5),
          mgp = c(2.5, 0.5, 0),
          tcl = -0.3)

plot(X.use, las = 1, asp = 1, xlab = "", ylab = "")
lines(circle.true, lwd = 2)
legend("topleft",
       legend = c("Observed samples", "True circle"),
       pch = c(1, NA),
       lty = c(NA, 1),
       col = c("black", "black"),
       bty = "n", cex = 0.9)

plot(X.use, las = 1, asp = 1, col = "gray", xlab = "", ylab = "")
lines(circle.true, lwd = 2)
points(X.smoothed, col = "blue", pch = 19, cex = 0.6)
legend("topleft",
       legend = c("Observed samples", "Smoothed samples", "True circle"),
       pch = c(1, 19, NA),
       lty = c(NA, NA, 1),
       col = c("black", "blue", "black"),
       bty = "n", cex = 0.85)
par(op)
```

```{r denoising-metric}
rad.obs <- sqrt(rowSums(X.use^2))
rad.sm <- sqrt(rowSums(X.smoothed^2))

rmse.obs <- sqrt(mean((rad.obs - radius)^2))
rmse.sm <- sqrt(mean((rad.sm - radius)^2))

data.frame(
  metric = c("RMSE radius (observed)", "RMSE radius (smoothed)"),
  value = c(rmse.obs, rmse.sm)
)
```

## 6) Smooth the Response on the Selected Graph

We reuse the graph/spectral structure selected by `data.smoother()` and apply
it to `y` via `refit.rdgraph.regression()`.

```{r fit-y}
fit.seed <- X.denoise.res$fit.best

y.fit <- refit.rdgraph.regression(
  fitted.model = fit.seed,
  y.new = as.double(y.use),
  per.column.gcv = TRUE,
  n.cores = 1L,
  verbose = FALSE
)

y.hat <- as.double(y.fit$fitted.values)

data.frame(
  metric = c("cor(y.hat, y.true)", "RMSE(y.hat, y.true)"),
  value = c(cor(y.hat, y.true.use),
            sqrt(mean((y.hat - y.true.use)^2)))
)
```

## Optional 3D Surface View

```{r layout-3d}
X.denoise.graph.layout.3d <- graph.embedding(
  adj.list = fit.seed$graph$adj.list,
  weights.list = fit.seed$graph$edge.length.list,
  invert.weights = TRUE,
  dim = 3,
  method = "fr"
)
```

```{r plot3d-optional, eval=FALSE}
if (requireNamespace("rgl", quietly = TRUE)) {
  plot3D.cont(
    X.denoise.graph.layout.3d,
    y.hat,
    radius = 0.03,
    legend.title = "y.hat"
  )
}
```

```{r plot3d-html-optional, eval=FALSE}
if (requireNamespace("rgl", quietly = TRUE) &&
    requireNamespace("htmlwidgets", quietly = TRUE)) {
  plot3D.cont.html(
    X.denoise.graph.layout.3d,
    y.hat,
    legend.title = "y.hat",
    output.file = NULL
  )
}
```

## 7) Build and Smooth Feature Functions

Features are built so some correlate with selected arcs of the response, while
others are pure noise.

```{r make-features}
set.seed(1002)

window.1 <- as.numeric(comp1.use >= quantile(comp1.use, probs = 0.60))
window.2 <- as.numeric(comp2.use >= quantile(comp2.use, probs = 0.60))

z.global <- y.true.use + rnorm(length(theta.use), sd = 0.08)
z.peak1.arc <- window.1 * y.true.use + rnorm(length(theta.use), sd = 0.08)
z.peak2.arc <- window.2 * y.true.use + rnorm(length(theta.use), sd = 0.08)
z.peak.contrast <- (comp1.use - comp2.use) + rnorm(length(theta.use), sd = 0.08)

z.noise1 <- rnorm(length(theta.use))
z.noise2 <- rnorm(length(theta.use))
z.noise3 <- rnorm(length(theta.use))

Z <- cbind(
  z_global = z.global,
  z_peak1_arc = z.peak1.arc,
  z_peak2_arc = z.peak2.arc,
  z_peak_contrast = z.peak.contrast,
  z_noise1 = z.noise1,
  z_noise2 = z.noise2,
  z_noise3 = z.noise3
)
Z <- scale(Z)
```

```{r smooth-z}
Z.fit <- refit.rdgraph.regression(
  fitted.model = fit.seed,
  y.new = Z,
  per.column.gcv = FALSE,
  n.cores = 1L,
  verbose = FALSE
)
Z.sm <- as.matrix(Z.fit$fitted.values)
summary(Z.fit)
```

## 8) Recover Extrema and Basins

```{r refined-basins}
y.basins <- compute.gfc(
  adj.list = fit.seed$graph$adj.list,
  edge.length.list = fit.seed$graph$edge.length.list,
  fitted.values = y.hat,
  edge.length.quantile.thld = 0.8,
  min.rel.value.max = 1.05,
  max.rel.value.min = 0.95,
  max.overlap.threshold = 0.20,
  min.overlap.threshold = 0.20,
  p.mean.nbrs.dist.threshold = 0.90,
  p.mean.hopk.dist.threshold = 0.90,
  p.deg.threshold = 0.90,
  min.basin.size = 8L,
  expand.basins = TRUE,
  apply.geometric.filter = TRUE,
  with.trajectories = TRUE,
  verbose = TRUE
)

y.basins$summary
table(y.basins$summary$type)
```

```{r compare-true-estimated-basins}
true.max.label <- ifelse(comp1.use >= comp2.use, "M1.true", "M2.true")

max.vertices.list <- y.basins$expanded.max.vertices.list
if (is.null(max.vertices.list)) max.vertices.list <- y.basins$max.vertices.list

est.max.label <- rep(NA_character_, length(y.hat))
for (lab in names(max.vertices.list)) {
  est.max.label[as.integer(max.vertices.list[[lab]])] <- lab
}

idx.ok <- !is.na(est.max.label)
table(True = true.max.label[idx.ok], Estimated = est.max.label[idx.ok])
```

## 9) Basin-Wise Pearson and Spearman Checks

```{r basin-wise-classical-correlation}
max.vertices.list <- y.basins$expanded.max.vertices.list
if (is.null(max.vertices.list)) max.vertices.list <- y.basins$max.vertices.list
min.vertices.list <- y.basins$expanded.min.vertices.list
if (is.null(min.vertices.list)) min.vertices.list <- y.basins$min.vertices.list

max.assign <- rep(NA_character_, length(y.hat))
for (lab in names(max.vertices.list)) {
  max.assign[as.integer(max.vertices.list[[lab]])] <- lab
}

min.assign <- rep(NA_character_, length(y.hat))
for (lab in names(min.vertices.list)) {
  min.assign[as.integer(min.vertices.list[[lab]])] <- lab
}

cell.assign <- ifelse(is.na(max.assign) | is.na(min.assign),
                      NA_character_,
                      paste(max.assign, min.assign, sep = "|"))

group.levels <- setdiff(sort(unique(max.assign)), NA_character_)
basin.cor <- data.frame()

for (j in seq_len(ncol(Z.sm))) {
  for (gr in group.levels) {
    idx <- which(max.assign == gr)
    if (length(idx) < 20L) next
    basin.cor <- rbind(
      basin.cor,
      data.frame(
        feature = colnames(Z.sm)[j],
        group = gr,
        n = length(idx),
        pearson = suppressWarnings(cor(y.hat[idx], Z.sm[idx, j], method = "pearson")),
        spearman = suppressWarnings(cor(y.hat[idx], Z.sm[idx, j], method = "spearman"))
      )
    )
  }
}

if (nrow(basin.cor) > 0) {
  head(basin.cor[order(-abs(basin.cor$spearman)), ], 12)
} else {
  basin.cor
}
```

## 10) Local Correlation with `lcor()`

```{r lcor-methods}
adj.list <- fit.seed$graph$adj.list
edge.length.list <- fit.seed$graph$edge.length.list

lcor.derivative <- as.matrix(lcor(adj.list, edge.length.list, y.hat, Z, type = "derivative"))
lcor.unit <- as.matrix(lcor(adj.list, edge.length.list, y.hat, Z, type = "unit"))
lcor.sign <- as.matrix(lcor(adj.list, edge.length.list, y.hat, Z, type = "sign"))

lcor.summary <- rbind(
  data.frame(
    feature = colnames(lcor.derivative),
    type = "derivative",
    mean.lcor = colMeans(lcor.derivative),
    mean.abs.lcor = colMeans(abs(lcor.derivative)),
    row.names = NULL
  ),
  data.frame(
    feature = colnames(lcor.unit),
    type = "unit",
    mean.lcor = colMeans(lcor.unit),
    mean.abs.lcor = colMeans(abs(lcor.unit)),
    row.names = NULL
  ),
  data.frame(
    feature = colnames(lcor.sign),
    type = "sign",
    mean.lcor = colMeans(lcor.sign),
    mean.abs.lcor = colMeans(abs(lcor.sign)),
    row.names = NULL
  )
)

lcor.summary[order(lcor.summary$type, -lcor.summary$mean.abs.lcor), ]
```

## 11) Permutation Inference for `lcor`

We assess feature significance by permuting feature rows (`permute = "z"`),
recomputing local correlations, and comparing observed statistics to this null.

```{r lcor-permutation}
perm.lcor <- permutation.test.lcor(
  adj.list = adj.list,
  weight.list = edge.length.list,
  y = y.hat,
  z = Z,
  type = "derivative",
  statistic = "mean.abs",
  permute = "z",
  n.perm = 200L,
  seed = 2026L
)

perm.lcor$table[order(perm.lcor$table$q.value), ]
```

## 12) Build an lcor-Based Feature Module Graph

```{r lcor-module-graph}
sim <- stats::cor(lcor.derivative, use = "pairwise.complete.obs")
adj.sim <- abs(sim)
diag(adj.sim) <- 0

threshold <- 0.20
adj.sim[adj.sim < threshold] <- 0

if (sum(adj.sim > 0) == 0) {
  message("No edges above threshold. Lower `threshold` to make the module graph denser.")
} else {
  g.mod <- igraph::graph_from_adjacency_matrix(
    adj.sim,
    mode = "undirected",
    weighted = TRUE,
    diag = FALSE
  )

  modules <- igraph::cluster_louvain(g.mod, weights = igraph::E(g.mod)$weight)

  igraph::plot.igraph(
    g.mod,
    vertex.label = igraph::V(g.mod)$name,
    vertex.size = 26,
    vertex.color = as.integer(igraph::membership(modules)) + 1,
    edge.width = 1 + 4 * igraph::E(g.mod)$weight,
    main = "Feature modules from local-correlation profiles"
  )

  data.frame(
    feature = igraph::V(g.mod)$name,
    module = as.integer(igraph::membership(modules)),
    row.names = NULL
  )
}
```

## Optional `grip` 3D Layout

```{r optional-grip-layout, eval=FALSE}
if (requireNamespace("grip", quietly = TRUE) &&
    requireNamespace("rgl", quietly = TRUE)) {
  g <- X.graphs$geom_pruned_graphs[[k.idx]]

  X.graphs.3d <- grip::grip.layout(
    adj_list = g$adj_list,
    weight_list = g$weight_list,
    dim = 3,
    rounds = 50,
    final_rounds = 50,
    num_init = 36,
    num_nbrs = 8,
    seed = 6
  )

  plot3D.plain(X.graphs.3d, radius = 0.03)
}
```

## Notes

- For publication-quality inference, increase permutation count (`n.perm`),
  report confidence intervals, and retain `q.value` (FDR-controlled) summaries.
- For larger datasets, use `n.cores > 1` and consider chunk-level caching.
- If basin refinement is unstable for a specific dataset, start with relaxed
  filtering and tighten thresholds gradually.
