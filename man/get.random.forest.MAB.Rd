% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bias_utils.R
\name{get.random.forest.MAB}
\alias{get.random.forest.MAB}
\title{Estimate Mean Absolute Bias (MAB) of Random Forest Model}
\usage{
get.random.forest.MAB(
  x,
  y,
  xt,
  yt,
  ntree = 500,
  optimize.ntree = FALSE,
  max.trees = 1000,
  plot.oob = FALSE,
  ...
)
}
\arguments{
\item{x}{A vector, matrix, or data frame of predictor values for training.}

\item{y}{A vector of response values (numeric for regression, factor for classification).}

\item{xt}{A vector, matrix, or data frame of predictor values for testing.}

\item{yt}{A vector of true response values for testing. For classification,
these should be probabilities of the positive class.}

\item{ntree}{Number of trees to grow in the random forest (default = 500).}

\item{optimize.ntree}{Logical. If TRUE, uses OOB error to find optimal number of trees
between 100 and max.trees (default = FALSE).}

\item{max.trees}{Maximum number of trees to consider when optimize.ntree = TRUE
(default = 1000).}

\item{plot.oob}{Logical. If TRUE and optimize.ntree = TRUE, plots the OOB error curve
(default = FALSE).}

\item{...}{Additional arguments passed to randomForest().}
}
\value{
A list containing:
\describe{
\item{MAB}{Mean Absolute Bias calculated on the test set}
\item{RMSE}{Root Mean Square Error}
\item{predictions}{Predictions made by the random forest model on the test set}
\item{residuals}{Absolute residuals}
\item{model}{The fitted random forest model object}
\item{parameters}{List containing ntree value used and optimal.ntree if optimization was performed}
}
}
\description{
Fits a Random Forest model to a given dataset and evaluates its performance
by computing the Mean Absolute Bias on test data. Supports both regression
and classification tasks with optional optimization of the number of trees.
}
\details{
For classification tasks (when y is a factor), the function returns
predicted probabilities for the second class level. For regression tasks,
it returns the predicted values directly.

When optimize.ntree = TRUE, the function fits a random forest with max.trees
and examines the OOB error curve to find where the error stabilizes. It selects
the smallest number of trees where the OOB error is within 1\% of the minimum.
}
\examples{
\dontrun{
# Regression example
set.seed(123)
x <- matrix(rnorm(200), ncol = 2)
y <- x[,1] + x[,2]^2 + rnorm(100, sd = 0.5)
xt <- matrix(rnorm(100), ncol = 2)
yt <- xt[,1] + xt[,2]^2

# Basic usage
result <- get.random.forest.MAB(x, y, xt, yt, ntree = 500)
print(result$MAB)

# With optimization
result.opt <- get.random.forest.MAB(x, y, xt, yt, optimize.ntree = TRUE)
print(result.opt$parameters$optimal.ntree)

# Classification example
y.class <- factor(ifelse(y > median(y), "high", "low"))
yt.prob <- pnorm(yt, mean = mean(yt), sd = sd(yt))
result.class <- get.random.forest.MAB(x, y.class, xt, yt.prob)
print(result.class$MAB)
}

}
