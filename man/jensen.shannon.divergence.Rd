% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{jensen.shannon.divergence}
\alias{jensen.shannon.divergence}
\title{Calculate Jensen-Shannon Divergence Between Two Probability Mass Functions}
\usage{
jensen.shannon.divergence(p, q)
}
\arguments{
\item{p}{Numeric vector representing the first probability mass function.
Will be normalized to sum to 1. Must not contain negative values.}

\item{q}{Numeric vector representing the second probability mass function.
Will be normalized to sum to 1. Must not contain negative values.}
}
\value{
A numeric value representing the Jensen-Shannon divergence in bits
(using log base 2). The value is always non-negative and bounded
above by 1.
}
\description{
Computes the Jensen-Shannon divergence (JSD) between two discrete probability
mass functions. The JSD is a symmetrized and smoothed version of the
Kullback-Leibler divergence. It is defined as:
JSD(P||Q) = 1/2 * D(P||M) + 1/2 * D(Q||M), where M = 1/2 * (P + Q)
and D denotes the Kullback-Leibler divergence.
}
\details{
The function implements the following behavior:
\itemize{
\item If input vectors have different lengths, the shorter vector is padded
with zeros to match the length of the longer vector
\item Input vectors are automatically normalized to sum to 1
\item Negative probabilities are not allowed
\item Zero probabilities are handled appropriately by only computing
the divergence over positive probability values
}

When vectors of different lengths are provided, the function effectively treats
the shorter distribution as having zero probability for the additional categories
present in the longer distribution. This allows comparison of distributions
with different support sizes while preserving the mathematical properties of
the Jensen-Shannon divergence.
}
\examples{
# Calculate JSD between two simple distributions
p <- c(0.4, 0.6)
q <- c(0.5, 0.5)
jensen.shannon.divergence(p, q)

# Calculate JSD between distributions of different lengths
p <- c(0.3, 0.7)
q <- c(0.2, 0.3, 0.5)
jensen.shannon.divergence(p, q)  # p will be padded with 0 to match q's length

# Calculate JSD between two categorical distributions
p <- c(0.2, 0.3, 0.5)
q <- c(0.1, 0.4, 0.5)
jensen.shannon.divergence(p, q)

}
\references{
Lin, J. (1991). Divergence measures based on the Shannon entropy.
IEEE Transactions on Information Theory, 37(1), 145-151.
}
