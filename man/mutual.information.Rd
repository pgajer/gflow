% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{mutual.information}
\alias{mutual.information}
\title{Estimate Mutual Information Between Dataset Membership and Features}
\usage{
mutual.information(X, Y, num.bins = 10)
}
\arguments{
\item{X}{A matrix or data frame representing the first dataset, where rows are observations
and columns are features. Must have the same number of columns as Y.}

\item{Y}{A matrix or data frame representing the second dataset, where rows are observations
and columns are features. Must have the same number of columns as X.}

\item{num.bins}{The number of bins to use for discretization (default is 10).}
}
\value{
A numeric value representing the estimated mutual information in nats (natural units).
Higher values indicate greater differences between the datasets. Returns 0 when the
datasets have identical distributions.
}
\description{
This function estimates the mutual information between the dataset membership (X or Y)
and the feature values. In the context of comparing two datasets, this mutual information
is equivalent to the Jensen-Shannon divergence, which is closely related to relative entropy.

Intuitively, mutual information measures how much knowing the features tells us about
which dataset a point came from, and vice versa. If the datasets are very different,
knowing the features will give us a lot of information about which dataset a point
belongs to, resulting in high mutual information.

The algorithm works by:
\enumerate{
\item Combining X and Y into a single dataset with labels.
\item Discretizing the continuous data.
\item Computing the mutual information between each feature and the dataset labels.
\item Summing these mutual information values.
}

This method is particularly useful when you want to understand which features contribute
most to the difference between the datasets, as you can look at the mutual information
for each feature separately.
}
\details{
The mutual information I(F;D) between features F and dataset label D is computed as:
\deqn{I(F;D) = \sum_{f,d} p(f,d) \log\frac{p(f,d)}{p(f)p(d)}}{I(F;D) = sum p(f,d) * log(p(f,d)/(p(f)*p(d)))}

The function uses the infotheo package's discretization method (equal frequency binning
by default) to handle continuous features. The total mutual information is the sum
across all features, which assumes feature independence given the dataset label.
}
\note{
\itemize{
\item Requires the 'infotheo' package to be installed
\item The discretization step can lose information, especially for highly continuous data
\item Results depend on the discretization method and number of bins used
\item For high-dimensional data, consider using only the most informative features
\item The assumption of summing MI across features may overestimate total information
if features are correlated
}
}
\examples{
# Example 1: Datasets with different means
set.seed(123)
X <- matrix(rnorm(1000), ncol = 2)
Y <- matrix(rnorm(1000, mean = 1), ncol = 2)
result <- mutual.information(X, Y)
print(paste("MI between datasets:", round(result, 4), "nats"))

# Example 2: Identical datasets (MI should be ~0)
X <- matrix(rnorm(1000), ncol = 2)
Y <- matrix(rnorm(1000), ncol = 2)
result <- mutual.information(X, Y)
print(paste("MI for identical distributions:", round(result, 4)))

# Example 3: Feature-wise contribution
X <- matrix(rnorm(500), ncol = 5)
Y <- X
Y[,3] <- Y[,3] + 2  # Only change feature 3
# Compute MI for each feature separately
\dontrun{
feature_mi <- sapply(1:5, function(i) {
  mutual.information(X[,i,drop=FALSE], Y[,i,drop=FALSE])
})
barplot(feature_mi, names.arg = paste("Feature", 1:5),
        main = "Feature-wise MI Contribution")
}
}
\references{
Cover, T. M., & Thomas, J. A. (2006). Elements of information theory. John Wiley & Sons.

Lin, J. (1991). Divergence measures based on the Shannon entropy.
IEEE Transactions on Information Theory, 37(1), 145-151.
}
\seealso{
\code{\link[infotheo]{mutinformation}} for the underlying MI computation,
\code{\link[infotheo]{discretize}} for the discretization method,
\code{total.variation.distance} for an alternative dataset comparison metric
}
