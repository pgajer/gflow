% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{entropy.difference}
\alias{entropy.difference}
\title{Estimate Relative Entropy Using Entropy Difference}
\usage{
entropy.difference(X, Y, num.bins = 10)
}
\arguments{
\item{X}{A matrix or data frame representing the first dataset.}

\item{Y}{A matrix or data frame representing the second dataset.}

\item{num.bins}{The number of bins to use for discretization (default is 10).}
}
\value{
A numeric value representing the estimated relative entropy.
}
\description{
This function estimates the relative entropy between two datasets by computing
the difference between the joint entropy and the average of individual entropies.
The intuition behind this method comes from the relationship between mutual
information and entropy:

I(X;Y) = H(X) + H(Y) - H(X,Y) = KL(P(X,Y) || P(X)P(Y))

Where I(X;Y) is the mutual information, H() is entropy, and KL() is the Kullback-Leibler divergence.

The algorithm works by:
\enumerate{
\item Discretizing the continuous data into bins.
\item Estimating the entropy of X, Y, and their union separately.
\item Computing the difference to approximate the relative entropy.
}

This method provides a rough approximation of the relative entropy and can be
useful when dealing with high-dimensional data where direct density estimation is challenging.
However, it's sensitive to the choice of binning and may not be as accurate as some other methods.
}
\examples{
X <- matrix(rnorm(1000), ncol = 2)
Y <- matrix(rnorm(1000, mean = 1), ncol = 2)
result <- entropy.difference(X, Y)
print(result)

}
\references{
Cover, T. M., & Thomas, J. A. (2006). Elements of information theory. John Wiley & Sons.
}
\seealso{
\code{\link[infotheo]{discretize}} for the discretization method,
}
