% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/spectral_lowess_graph_smoothing.R
\name{spectral.lowess.graph.smoothing}
\alias{spectral.lowess.graph.smoothing}
\title{Iterative Spectral LOWESS Graph Smoothing}
\usage{
spectral.lowess.graph.smoothing(
  adj.list,
  weight.list,
  X,
  max.iterations = 10,
  convergence.threshold = 1e-04,
  convergence.type = 1,
  k = 10,
  pruning.thld = 0.1,
  n.evectors = 8,
  n.bws = 10,
  log.grid = TRUE,
  min.bw.factor = 0.05,
  max.bw.factor = 0.5,
  dist.normalization.factor = 1.1,
  kernel.type = 7L,
  n.cleveland.iterations = 1,
  compute.errors = TRUE,
  compute.scales = TRUE,
  switch.to.residuals.after = NULL,
  verbose = FALSE
)
}
\arguments{
\item{adj.list}{A list of integer vectors representing the adjacency list of
the graph. Each element \code{adj.list[[i]]} contains the indices of vertices
adjacent to vertex \code{i}. Indices must be between 1 and \code{length(adj.list)}.}

\item{weight.list}{A list of numeric vectors containing edge weights corresponding
to the adjacencies. Each element \code{weight.list[[i]][j]} is the weight of
the edge from vertex \code{i} to vertex \code{adj.list[[i]][j]}. Weights must
be non-negative. Must have the same structure as \code{adj.list}.}

\item{X}{A numeric matrix where rows represent samples/vertices and columns
represent features. Must have \code{nrow(X) == length(adj.list)}. Missing
values are not allowed.}

\item{max.iterations}{Maximum number of iterations to perform. Must be a positive
integer. Default: 10.}

\item{convergence.threshold}{Threshold for convergence. The algorithm stops when
the convergence metric falls below this value. Must be positive. Default: 1e-4.}

\item{convergence.type}{Type of convergence criterion to use:
\itemize{
\item{1: Maximum absolute difference between successive iterations}
\item{2: Mean absolute difference between successive iterations}
\item{3: Maximum relative change between successive iterations}
}
Default: 1.}

\item{k}{Number of nearest neighbors for k-NN graph construction. Must be a
positive integer less than the number of vertices. Default: 10.}

\item{pruning.thld}{Threshold for pruning edges in graph construction. Edges
with weights below this threshold are removed. Must be non-negative. Default: 0.1.}

\item{n.evectors}{Number of eigenvectors to use for spectral embedding. Must be
a positive integer. Larger values capture more graph structure but increase
computation time. Default: 8.}

\item{n.bws}{Number of candidate bandwidths to consider for LOWESS smoothing.
Must be a positive integer. Default: 10.}

\item{log.grid}{Logical. If \code{TRUE}, use logarithmic spacing for the bandwidth
grid; if \code{FALSE}, use linear spacing. Default: \code{TRUE}.}

\item{min.bw.factor}{Factor for minimum bandwidth, multiplied by the graph diameter
to determine the actual minimum bandwidth. Must be positive and less than
\code{max.bw.factor}. Default: 0.05.}

\item{max.bw.factor}{Factor for maximum bandwidth, multiplied by the graph diameter
to determine the actual maximum bandwidth. Must be positive and greater than
\code{min.bw.factor}. Default: 0.5.}

\item{dist.normalization.factor}{Factor for normalizing distances in kernel weight
calculation. Must be at least 1.1. Higher values result in more uniform weights.
Default: 1.1.}

\item{kernel.type}{Integer specifying the kernel function for weighting:
\itemize{
\item{7: Gaussian kernel (default)}
\item{Other values may be supported depending on the C++ implementation}
}
Default: 7L.}

\item{n.cleveland.iterations}{Number of robustness iterations for Cleveland's
LOWESS algorithm. Must be a non-negative integer. Default: 1.}

\item{compute.errors}{Logical. If \code{TRUE}, compute prediction errors for
each iteration. May increase computation time. Default: \code{TRUE}.}

\item{compute.scales}{Logical. If \code{TRUE}, compute bandwidth/scale information
for each iteration. May increase computation time. Default: \code{TRUE}.}

\item{switch.to.residuals.after}{Number of iterations to perform direct smoothing
before switching to residual smoothing (boosting mode). Must be a non-negative
integer. Set to 0 to use residual smoothing from the start. If \code{NULL},
defaults to \code{max.iterations} (never switch). Default: \code{NULL}.}

\item{verbose}{Logical. If \code{TRUE}, print progress information during iterations.
Default: \code{FALSE}.}
}
\value{
A list of class \code{"spectral.lowess.result"} containing:
\item{smoothed.graphs}{A list of length \code{iterations.performed}, where each
element is a graph (represented as adjacency and weight lists) after
the corresponding iteration}
\item{smoothed.X}{A list of length \code{iterations.performed}, where each
element is the smoothed data matrix after the corresponding iteration}
\item{convergence.metrics}{A numeric vector of length \code{iterations.performed}
containing the convergence metric at each iteration}
\item{iterations.performed}{Integer indicating the number of iterations actually
performed before convergence or reaching \code{max.iterations}}
\item{used.boosting}{Logical indicating whether boosting (residual smoothing)
was used in any iteration}
\item{call}{The matched function call}
\item{parameters}{A list containing all input parameters for reproducibility}
}
\description{
Applies iterative spectral LOWESS (Locally Weighted Scatterplot Smoothing) to
graph-structured data. The algorithm iteratively smooths features by estimating
conditional expectations using spectral embeddings of local neighborhoods,
then reconstructs the graph from the smoothed data.
}
\details{
The algorithm performs the following steps iteratively:
\enumerate{
\item Compute spectral embedding of the graph using eigenvectors of the
normalized Laplacian
\item For each vertex, estimate conditional expectations of features using
LOWESS on the spectral embedding coordinates
\item Reconstruct the graph from the smoothed features using k-NN with
specified pruning
\item Check convergence criterion and stop if threshold is met
}

The boosting mode (residual smoothing) can be activated by setting
\code{switch.to.residuals.after} to a value less than \code{max.iterations}.
In this mode, the algorithm smooths residuals from previous iterations rather
than the data directly, which can improve convergence in some cases.
}
\note{
\itemize{
\item The function requires a C++ implementation accessed via \code{.Call}
\item Large graphs or high-dimensional data may require substantial memory
\item The choice of convergence type affects both speed and quality of results
\item Edge weights should typically be similarity measures (larger = more similar)
}
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
n <- 100
p <- 10
X <- matrix(rnorm(n * p), nrow = n, ncol = p)

# Create a k-NN graph
library(FNN)
knn.result <- get.knn(X, k = 10)
adj.list <- lapply(1:n, function(i) knn.result$nn.index[i, ])
weight.list <- lapply(1:n, function(i) 1 / (1 + knn.result$nn.dist[i, ]))

# Apply spectral LOWESS smoothing
result <- spectral.lowess.graph.smoothing(
  adj.list = adj.list,
  weight.list = weight.list,
  X = X,
  max.iterations = 5,
  convergence.threshold = 1e-3,
  verbose = TRUE
)

# Examine results
cat("Iterations performed:", result$iterations.performed, "\n")
cat("Final convergence metric:",
    result$convergence.metrics[result$iterations.performed], "\n")

# Compare original and smoothed data
X.smoothed <- result$smoothed.X[[result$iterations.performed]]
par(mfrow = c(1, 2))
image(X, main = "Original Data", xlab = "Sample", ylab = "Feature")
image(X.smoothed, main = "Smoothed Data", xlab = "Sample", ylab = "Feature")

# Example with boosting
result.boost <- spectral.lowess.graph.smoothing(
  adj.list = adj.list,
  weight.list = weight.list,
  X = X,
  max.iterations = 10,
  switch.to.residuals.after = 3,
  verbose = TRUE
)

# Compare convergence
plot(result$convergence.metrics, type = "b", col = "blue",
     xlab = "Iteration", ylab = "Convergence Metric",
     main = "Convergence Comparison")
lines(result.boost$convergence.metrics, type = "b", col = "red")
legend("topright", legend = c("Standard", "Boosting"),
       col = c("blue", "red"), lty = 1, pch = 1)
}

}
\references{
Cleveland, W. S. (1979). Robust locally weighted regression and smoothing
scatterplots. \emph{Journal of the American Statistical Association},
74(368), 829-836.
}
