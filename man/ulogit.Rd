% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/ulogit.R
\name{ulogit}
\alias{ulogit}
\title{Fit Univariate Logistic Regression Model}
\usage{
ulogit(
  x,
  y,
  w = NULL,
  max.iterations = 100L,
  ridge.lambda = 0.002,
  max.beta = 100,
  tolerance = 1e-08,
  verbose = FALSE
)
}
\arguments{
\item{x}{Numeric vector of predictor values. Must contain only finite values
(no NA, NaN, or Inf).}

\item{y}{Binary response vector containing only 0 or 1 values. Must be the
same length as \code{x}.}

\item{w}{Optional numeric vector of non-negative observation weights. If
\code{NULL} (default), all observations are given equal weight. Must be
the same length as \code{x} if provided.}

\item{max.iterations}{Maximum number of iterations for Newton-Raphson
optimization. Must be a positive integer. Default is 100.}

\item{ridge.lambda}{Ridge regularization parameter to improve numerical
stability. Must be non-negative. Default is 0.002. Higher values provide
more regularization but may increase bias.}

\item{max.beta}{Maximum allowed absolute value for coefficient estimates.
Used to prevent numerical overflow. Must be positive. Default is 100.0.}

\item{tolerance}{Convergence tolerance for optimization. The algorithm stops
when the relative change in log-likelihood is less than this value. Must
be positive. Default is 1e-8.}

\item{verbose}{Logical flag to enable detailed output during optimization.
If \code{TRUE}, prints iteration progress. Default is \code{FALSE}.}
}
\value{
A list of class \code{"ulogit"} containing:
\describe{
\item{predictions}{Numeric vector of fitted probabilities for each observation}
\item{errors}{Numeric vector of leave-one-out cross-validation errors}
\item{weights}{Numeric vector of weights used in model fitting}
\item{converged}{Logical indicating whether the algorithm converged}
\item{iterations}{Integer giving the number of iterations used}
\item{call}{The matched call}
}
}
\description{
Fits a univariate logistic regression model to binary response data using
Newton-Raphson optimization with optional ridge regularization. The function
implements numerical safeguards to ensure stable convergence and provides
leave-one-out cross-validation errors for model assessment.
}
\details{
The function fits a logistic regression model of the form:
\deqn{logit(p_i) = \beta_0 + \beta_1 x_i}
where \eqn{p_i} is the probability of success for observation \eqn{i}.

The Newton-Raphson algorithm is used for maximum likelihood estimation with
optional ridge regularization. The ridge penalty adds \eqn{\lambda \beta^2}
to the negative log-likelihood, which helps stabilize the estimation when
the data are nearly separable.

Leave-one-out cross-validation (LOOCV) errors are computed efficiently using
the hat matrix diagonal elements, providing a measure of predictive accuracy
without requiring repeated model fitting.
}
\examples{
# Basic usage with simulated data
set.seed(123)
x <- seq(0, 1, length.out = 100)
true_prob <- 1/(1 + exp(-(2*x - 1)))
y <- rbinom(100, 1, prob = true_prob)
fit <- ulogit(x, y)

# Plot results
plot(x, y, pch = 16, col = ifelse(y == 1, "blue", "red"),
     main = "Univariate Logistic Regression")
lines(x, fit$predictions, lwd = 2)
legend("topleft", c("y = 1", "y = 0", "Fitted"),
       col = c("blue", "red", "black"),
       pch = c(16, 16, NA), lty = c(NA, NA, 1))

# Example with weights
w <- runif(100, 0.5, 1.5)
fit_weighted <- ulogit(x, y, w = w)

# Example with increased regularization
fit_regularized <- ulogit(x, y, ridge.lambda = 0.1)

# Compare LOOCV errors
cat("Standard model LOOCV error:", mean(fit$errors), "\n")
cat("Weighted model LOOCV error:", mean(fit_weighted$errors), "\n")
cat("Regularized model LOOCV error:", mean(fit_regularized$errors), "\n")

}
\references{
Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of
Statistical Learning (2nd ed.). Springer.
}
\seealso{
\code{\link{eigen.ulogit}} for an alternative implementation using Eigen,
\code{\link[stats]{glm}} for multivariate logistic regression
}
