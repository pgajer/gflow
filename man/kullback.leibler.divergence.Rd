% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{kullback.leibler.divergence}
\alias{kullback.leibler.divergence}
\title{Calculate Kullback-Leibler Divergence Between Two Probability Mass Functions}
\usage{
kullback.leibler.divergence(p, q, zero.handling = "pseudo", epsilon = 1e-10)
}
\arguments{
\item{p}{Numeric vector representing the first probability mass function \code{P}.
Will be normalized to sum to 1. Must not contain negative values.}

\item{q}{Numeric vector representing the second probability mass function \code{Q}.
Will be normalized to sum to 1. Must not contain negative values.}

\item{zero.handling}{Character string specifying how to handle zeros in \code{Q}.
Must be one of \code{"strict"} (error if \code{Q[i] = 0} where \code{P[i] > 0}),
\code{"exclude"} (compute only where \code{Q > 0}), or
\code{"pseudo"} (add small constant to \code{Q}). Defaults to \code{"pseudo"}.}

\item{epsilon}{Numeric value specifying the small constant to add when
\code{zero.handling = "pseudo"}. Defaults to \code{1e-10}.}
}
\value{
A numeric value representing the Kullback-Leibler divergence in bits
(using log base 2). The value is always non-negative, and equals
infinity if there exists an \code{i} where \code{P[i] > 0} and \code{Q[i] = 0}
(when \code{zero.handling = "strict"}).
}
\description{
Computes the Kullback-Leibler divergence (KL divergence) between two discrete
probability mass functions. The KL divergence is a measure of the information
lost when \code{Q} is used to approximate \code{P}. Note that KL divergence is not symmetric:
\eqn{\text{KL}(P||Q) \ne \text{KL}(Q||P)} in general.
}
\details{
The function implements the following behavior:
\itemize{
\item If input vectors have different lengths, the shorter vector is padded
with zeros to match the length of the longer vector
\item Input vectors are automatically normalized to sum to 1
\item Negative probabilities are not allowed
}
The implementation provides three ways to handle zeros in the \code{Q} distribution:
\itemize{
\item \code{"strict"}: Throws an error if \code{Q[i] = 0} where \code{P[i] > 0}
\item \code{"exclude"}: Computes divergence only over indices where \code{Q > 0}
\item \code{"pseudo"}: Adds small constant \code{epsilon} to \code{Q} (and renormalizes)
}

When vectors of different lengths are provided, the function treats the shorter
distribution as having zero probability for the additional categories present
in the longer distribution. This is particularly important when using
"strict" zero handling, as padding with zeros could trigger errors if P
is the shorter vector.
}
\examples{
# Calculate KL divergence between two simple distributions
p <- c(0.4, 0.6)
q <- c(0.5, 0.5)
kullback.leibler.divergence(p, q)

# Using different zero handling methods
p <- c(0.2, 0.8, 0)
q <- c(0.3, 0.6, 0.1)
kullback.leibler.divergence(p, q, zero.handling = "exclude")
kullback.leibler.divergence(p, q, zero.handling = "pseudo")

# Calculate KL divergence between distributions of different lengths
p <- c(0.3, 0.7)
q <- c(0.2, 0.3, 0.5)
kullback.leibler.divergence(p, q)  # p will be padded with 0 to match q's length

}
\references{
Kullback, S., & Leibler, R. A. (1951). On information and sufficiency.
The Annals of Mathematical Statistics, 22(1), 79-86.
}
