% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{nn.distance.ratio.estimator}
\alias{nn.distance.ratio.estimator}
\title{Estimate Relative Entropy Using Nearest Neighbor Distance Ratio}
\usage{
nn.distance.ratio.estimator(X, Y, k = 1, eps = NULL, eps.factor = 1e-08)
}
\arguments{
\item{X}{A matrix or data frame representing the first dataset.}

\item{Y}{A matrix or data frame representing the second dataset.}

\item{k}{The number of nearest neighbors to consider (default is 1).}

\item{eps}{A small constant to add to distances to avoid numerical issues (default is NULL, which means it will be automatically determined).}

\item{eps.factor}{A small factor used to compute eps when eps is NULL. eps is set to the smallest non-zero distance multiplied by this factor (default is 1e-8).}
}
\value{
A numeric value representing the estimated relative entropy.
}
\description{
This function estimates the relative entropy (Kullback-Leibler divergence) between two datasets
using the nearest neighbor distance ratio method. It includes safeguards against zero distances
and potential numerical instabilities.
}
\examples{
X <- matrix(rnorm(1000), ncol = 2)
Y <- matrix(rnorm(1000, mean = 1), ncol = 2)
result <- nn.distance.ratio.estimator(X, Y)
print(result)

}
\references{
Wang, Q., Kulkarni, S. R., & Verdu, S. (2009). Divergence estimation for multidimensional densities
via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5), 2392-2405.
}
