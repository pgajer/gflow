% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/divergences.R
\name{init.version.nn.distance.ratio.estimator}
\alias{init.version.nn.distance.ratio.estimator}
\title{Estimate Relative Entropy Using Nearest Neighbor Distance Ratio}
\usage{
init.version.nn.distance.ratio.estimator(X, Y, k = 1)
}
\arguments{
\item{X}{A matrix or data frame representing the first dataset.}

\item{Y}{A matrix or data frame representing the second dataset.}

\item{k}{The number of nearest neighbors to consider (default is 1).}
}
\value{
A numeric value representing the estimated relative entropy.
}
\description{
This function estimates the relative entropy (Kullback-Leibler divergence) between two datasets
using the nearest neighbor distance ratio method. The intuition behind this approach is that
if two distributions are similar, the ratio of distances to nearest neighbors within a set
and to the other set should be close to 1. If the distributions differ, this ratio will deviate from 1.

The algorithm works by:
\enumerate{
\item Finding the k-nearest neighbors for each point within its own set and in the other set.
\item Computing the ratios of these distances.
\item Using the log of these ratios to estimate the relative entropy.
}

This method is particularly useful in high-dimensional spaces where traditional density
estimation techniques may fail due to the curse of dimensionality.
}
\examples{
X <- matrix(rnorm(1000), ncol = 2)
Y <- matrix(rnorm(1000, mean = 1), ncol = 2)
result <- nn.distance.ratio.estimator(X, Y)
print(result)

}
\references{
Wang, Q., Kulkarni, S. R., & VerdÃº, S. (2009). Divergence estimation for multidimensional densities
via k-nearest-neighbor distances. IEEE Transactions on Information Theory, 55(5), 2392-2405.
}
